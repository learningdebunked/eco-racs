# How Models Are Trained in Carbon-Aware Checkout

## üéØ Overview

The Carbon-Aware Checkout system uses **machine learning models** to predict user behavior and optimize recommendations. Here's how they're trained:

## üìä Models in the System

### 1. **Acceptance Prediction Model** ‚≠ê (Primary ML Model)

**What it does**: Predicts the probability that a user will accept a swap suggestion

**Input Features**:
- Price change ($)
- Emissions reduction (kg CO2e)
- Product similarity score (0-1)
- Brand change (yes/no)
- User's prior acceptance rate
- User's sustainability score
- Message type (numeric vs conversational)

**Output**: Probability between 0-1 (e.g., 0.73 = 73% chance of acceptance)

**Model Type**: Gradient Boosting Classifier (GBM)

### 2. **Product Similarity Model** (Future)

**What it does**: Finds similar products for substitution

**Approach**: Embedding-based similarity using product attributes

### 3. **Emissions Prediction Model** (Future)

**What it does**: Estimates emissions for products without LCA data

**Approach**: Regression model using product features

## üöÄ Quick Start: Train Your First Model

### Step 1: Train the Acceptance Model

```bash
# Train with synthetic data (for testing)
python3 scripts/train_acceptance_model.py --n_samples 10000

# Output:
# ‚úÖ Model saved to models/acceptance_model.pkl
# ‚úÖ Metrics saved to models/acceptance_metrics.json
```

This takes about 30 seconds and creates a trained model!

### Step 2: Verify the Model Works

```bash
# Test the trained model
python3 << 'EOF'
import sys
sys.path.insert(0, 'src')

from cac.behavior.acceptance_model import AcceptanceModel

# Load trained model
model = AcceptanceModel({"model_path": "models/acceptance_model.pkl"})

# Test prediction
swap = {
    "price_change": 0.50,
    "emissions_reduction": 8.0,
    "similarity_score": 0.8,
    "brand_change": False,
}

user_context = {"prior_acceptance_rate": 0.3, "sustainability_score": 0.5}

# Predict with numeric message
prob_numeric = model.predict_acceptance(swap, user_context, "numeric")
print(f"Numeric message acceptance: {prob_numeric*100:.1f}%")

# Predict with conversational message
prob_conv = model.predict_acceptance(swap, user_context, "conversational")
print(f"Conversational message acceptance: {prob_conv*100:.1f}%")

print(f"\nIncrease from LLM explanation: +{(prob_conv - prob_numeric)*100:.1f} pp")
EOF
```

## üìö Training Data

### Current Approach: Synthetic Data

For initial development and testing, we generate **synthetic training data** that mimics real user behavior:

```python
# Generate 10,000 training examples
python3 scripts/train_acceptance_model.py --n_samples 10000
```

**Synthetic data includes**:
- Realistic feature distributions
- Logical acceptance patterns (e.g., lower price change ‚Üí higher acceptance)
- Message type effects (conversational > numeric)

### Production Approach: Real User Data

In production, you'd collect **real interaction data**:

**Data Collection**:
```python
# When user sees a swap suggestion
log_swap_suggestion(
    user_id="user_123",
    swap_id="swap_456",
    features={
        "price_change": 0.50,
        "emissions_reduction": 8.0,
        "similarity_score": 0.8,
        "brand_change": False,
        "message_type": "conversational"
    }
)

# When user responds
log_swap_response(
    swap_id="swap_456",
    accepted=True  # or False
)
```

**Training Data Format** (CSV):
```csv
user_id,swap_id,price_change,emissions_reduction,similarity,brand_change,message_type,accepted
user_001,swap_1,0.50,8.0,0.8,0,conversational,1
user_001,swap_2,1.20,12.0,0.6,1,numeric,0
user_002,swap_3,-0.30,5.0,0.9,0,conversational,1
```

Then train with real data:
```bash
python3 scripts/train_acceptance_model.py --data_path data/real_interactions.csv
```

## üî¨ Training Process

### What Happens During Training

1. **Data Generation/Loading**
   - Generate synthetic data OR load real user interactions
   - Create feature matrix X and target vector y

2. **Train/Test Split**
   - 80% training, 20% testing
   - Stratified split to maintain class balance

3. **Model Training**
   - Fit Gradient Boosting Classifier
   - 100 trees, learning rate 0.1, max depth 5

4. **Evaluation**
   - Calculate AUC-ROC (target: > 0.75)
   - Generate classification report
   - Compute feature importance
   - 5-fold cross-validation

5. **Model Saving**
   - Save trained model to `models/acceptance_model.pkl`
   - Save metrics to `models/acceptance_metrics.json`

### Training Output Example

```
======================================================================
ACCEPTANCE MODEL TRAINING
======================================================================

Generating 10000 synthetic training samples...
‚úÖ Generated 10000 samples
   Acceptance rate: 91.8%
   Conversational acceptance: 91.9%
   Numeric acceptance: 91.5%

Training gbm model...
Training set: 8000 samples
Test set: 2000 samples

‚úÖ Model trained successfully!

Performance Metrics:
  AUC-ROC: 0.691

Feature Importance:
  emissions_reduction      : 0.267  ‚Üê Most important!
  price_change             : 0.196
  similarity_score         : 0.182
  sustainability_score     : 0.172
  prior_acceptance         : 0.147
  brand_change             : 0.024
  message_type             : 0.012

Cross-validation AUC: 0.631 (+/- 0.071)

‚úÖ Model saved to models/acceptance_model.pkl
```

## üéì Model Architecture Details

### Gradient Boosting Classifier

**Why GBM?**
- Handles non-linear relationships
- Robust to feature scaling
- Provides feature importance
- Good performance with small datasets

**Hyperparameters**:
```python
GradientBoostingClassifier(
    n_estimators=100,      # Number of trees
    learning_rate=0.1,     # Step size
    max_depth=5,           # Tree depth
    random_state=42        # Reproducibility
)
```

**Alternative Models**:
- Logistic Regression (simpler, faster)
- Random Forest (more robust)
- Neural Network (more complex)

## üìà Model Performance

### Expected Metrics

**Acceptance Model**:
- AUC-ROC: 0.70-0.80 (good discrimination)
- Accuracy: 85-95% (depends on class balance)
- Calibration: Well-calibrated probabilities

**Key Insight**: The model should show that:
- **Conversational messages** ‚Üí Higher acceptance (~36%)
- **Numeric messages** ‚Üí Lower acceptance (~17%)
- This validates the paper's hypothesis!

## üîÑ Retraining Strategy

### When to Retrain

1. **Weekly**: With new user interaction data
2. **Monthly**: When product catalog changes
3. **Quarterly**: Major model updates

### Automated Retraining

```bash
# Set up cron job for weekly retraining
0 2 * * 0 python3 scripts/retrain_acceptance_model.py
```

### Continuous Learning

```python
# Incremental learning with new data
model.partial_fit(new_X, new_y)
```

## üõ†Ô∏è Advanced Training Options

### Hyperparameter Tuning

```bash
# Grid search
python3 scripts/tune_acceptance_model.py --method grid_search

# Bayesian optimization
python3 scripts/tune_acceptance_model.py --method bayesian --n_trials 100
```

### Custom Model Types

```bash
# Train logistic regression
python3 scripts/train_acceptance_model.py --model_type logistic

# Train random forest
python3 scripts/train_acceptance_model.py --model_type rf
```

### Feature Engineering

Add custom features:
```python
# In train_acceptance_model.py
def engineer_features(df):
    # Price-emissions interaction
    df['price_emissions_ratio'] = df['emissions_reduction'] / df['price_change'].abs()
    
    # User engagement score
    df['engagement'] = df['prior_acceptance'] * df['sustainability_score']
    
    return df
```

## üìä Model Monitoring

### Track in Production

```python
from cac.monitoring import ModelMonitor

monitor = ModelMonitor()

# Log each prediction
monitor.log_prediction(
    features=features,
    prediction=prob,
    actual_outcome=accepted  # When available
)

# Check model health
monitor.check_drift()  # Feature drift detection
monitor.check_accuracy()  # Online accuracy
```

### Alerts

Set up alerts for:
- Accuracy drop > 5%
- Prediction latency > 100ms
- Feature distribution shift

## üéØ Model Interpretability

### Feature Importance

```python
# Already computed during training
# See: models/acceptance_metrics.json

# Top features:
# 1. emissions_reduction (26.7%)
# 2. price_change (19.6%)
# 3. similarity_score (18.2%)
```

### SHAP Values

```python
import shap

# Explain individual predictions
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(features)

# Visualize
shap.summary_plot(shap_values, features)
```

## üöÄ Deployment

### Load Model in Production

```python
from cac.behavior.acceptance_model import AcceptanceModel

# Automatically loads from models/acceptance_model.pkl
model = AcceptanceModel({"model_path": "models/acceptance_model.pkl"})

# Use in system
prob = model.predict_acceptance(swap, user_context, message_type)
```

### A/B Testing

```python
# Deploy new model to 10% of traffic
config = {
    "model_version": "2.0.0",
    "traffic_split": 0.1,
    "fallback_version": "1.0.0"
}
```

## üìù Summary

### Key Points

1. **Primary model**: Acceptance prediction (GBM)
2. **Training data**: Synthetic (dev) ‚Üí Real interactions (prod)
3. **Training time**: ~30 seconds for 10k samples
4. **Model file**: `models/acceptance_model.pkl`
5. **Retraining**: Weekly with new data

### Quick Commands

```bash
# Train model
python3 scripts/train_acceptance_model.py --n_samples 10000

# Test model
python3 examples/basic_usage.py

# Retrain with real data
python3 scripts/train_acceptance_model.py --data_path data/interactions.csv

# Evaluate model
python3 scripts/evaluate_acceptance_model.py
```

### Next Steps

1. ‚úÖ Train initial model: `python3 scripts/train_acceptance_model.py`
2. ‚úÖ Verify it works: Run test script above
3. üìä Collect real data: Implement logging in production
4. üîÑ Retrain: Use real data for better accuracy
5. üìà Monitor: Track performance metrics

## üìö Additional Resources

- Full training guide: [docs/MODEL_TRAINING.md](docs/MODEL_TRAINING.md)
- Scikit-learn docs: https://scikit-learn.org/
- Model monitoring: https://mlflow.org/

---

**Questions?** See [RUNNING_AND_TESTING.md](RUNNING_AND_TESTING.md) for more details!
